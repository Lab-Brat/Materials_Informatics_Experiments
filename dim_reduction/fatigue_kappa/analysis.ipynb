{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Materials Fatigues data has the shape:  (437, 16) and (437,)\nThermal Conductance data has the shape: (320, 36) and (320,)\n"
     ]
    }
   ],
   "source": [
    "# read MATERIALS FATIGUE data from csv file\n",
    "data = pd.read_csv('fatigue_data.csv', index_col='Sl. No.')\n",
    "\n",
    "# set X as columns C, Ni, Cr and Mo (No.16-19)\n",
    "Xf = data.drop(data.columns[16:20], axis=1) \n",
    "# set y as 'Fatigue' column (No.17)\n",
    "yf = data['Fatigue']\n",
    "\n",
    "\n",
    "# read THERMAL CONDUCTANCE data from xlsx file\n",
    "data = pd.read_excel('kappa.xlsx')\n",
    "\n",
    "Xk = data.drop(data.columns[0:3], axis=1).drop(data.columns[-1], axis=1)\n",
    "yk = data[data.columns[-1]]\n",
    "\n",
    "\n",
    "# Output shapes\n",
    "print(\"Materials Fatigues data has the shape:  {0} and {1}\".format(Xf.shape, yf.shape))\n",
    "print(\"Thermal Conductance data has the shape: {0} and {1}\".format(Xk.shape, yk.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance Threshold\n",
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "VT result for Material Fatigue\n(437, 10)\n\n\nVT result for Kappa\n(320, 12)\n"
     ]
    }
   ],
   "source": [
    "thresholder = VarianceThreshold(threshold=100)\n",
    "print(\"VT result for Material Fatigue\")\n",
    "Xf_features_high_variance = thresholder.fit_transform(Xf)\n",
    "print(Xf_features_high_variance.shape)\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "thresholder = VarianceThreshold(threshold=100)\n",
    "print(\"VT result for Kappa\")\n",
    "Xk_features_high_variance = thresholder.fit_transform(Xk)\n",
    "print(Xk_features_high_variance.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "features=[[0,1,0],\n",
    "[0,1,1],\n",
    "[0,1,0],\n",
    "[0,1,1],\n",
    "[1,0,0]]\n",
    "\n",
    "thresholder = VarianceThreshold(0.75 * (1 - 0.75))\n",
    "features_high_variance = thresholder.fit_transform(features)\n",
    "features_high_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.16, 0.16, 0.24])"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "thresholder.fit(features).variances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "VT result for Material Fatigue\n(437, 14)\n\n\n"
     ]
    }
   ],
   "source": [
    "thresholder = VarianceThreshold(threshold=.9 * (1 - .9))\n",
    "print(\"VT result for Material Fatigue\")\n",
    "Xf_features_high_variance = thresholder.fit_transform(Xf)\n",
    "print(Xf_features_high_variance.shape)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Variance of Fatugue Data\n[6.85500526e+02 7.82410122e+04 1.05105017e+02 6.13474229e+01\n 7.91977756e+04 1.60730141e+04 7.11945768e+04 2.45928321e+02\n 3.76245359e+02 2.68678309e+04 4.60233860e+02 6.50114102e+01\n 9.26471103e-03 7.25902728e-01 1.69165923e-01 7.74801250e-03]\n\n\nVariance of Kappa Data\n[2.47646484e-01 5.31839844e+00 4.78655859e+01 1.54351715e+03\n 8.37815954e-01 8.31401508e-01 1.95799691e+00 1.53808594e+02\n 1.53808594e+02 4.74750000e+02 3.54020194e+00 3.18413379e+03\n 2.47495301e+02 3.36530771e+00 8.63942685e+01 4.14071787e+02\n 1.54032412e+02 1.70491485e+02 1.58746484e-01 1.16105371e-01\n 6.09313283e-02 1.15121094e+00 1.05389648e+00 6.35691904e-01\n 7.19996094e+00 1.45183496e+01 8.33767374e+00 2.86790430e+03\n 8.70593332e+02 1.09367015e+03 6.54301277e-01 2.10370207e-01\n 2.16011763e-01 4.19964840e-01 1.22278827e-01 1.33775910e-01]\n"
     ]
    }
   ],
   "source": [
    "print(\"Variance of Fatugue Data\")\n",
    "print(thresholder.fit(Xf).variances_)\n",
    "print('\\n')\n",
    "\n",
    "print(\"Variance of Kappa Data\")\n",
    "print(thresholder.fit(Xk).variances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "Xf_std = scaler.fit_transform(Xf)\n",
    "\n",
    "selector = VarianceThreshold()\n",
    "\n",
    "# output 1 means that feature noramlization is scuccessful\n",
    "selector.fit(Xf_std).variances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1.])"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "Xk_std = scaler.fit_transform(Xk)\n",
    "\n",
    "selector = VarianceThreshold()\n",
    "\n",
    "# output 1 means that feature noramlization is scuccessful\n",
    "selector.fit(Xk_std).variances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Before Scaling\nMaxes: 389.634915603905\nMins: -82.56689644\n\n\nAfter Scaling\nMaxes: 6.539535537180673\nMins: -5.476052162160264\n"
     ]
    }
   ],
   "source": [
    "MAX_b = []\n",
    "MIN_b = []\n",
    "\n",
    "for j in range(len(Xk.columns)):\n",
    "    MAX_b.append(Xk[Xk.columns[j]].max())\n",
    "    MIN_b.append(Xk[Xk.columns[j]].min())\n",
    "\n",
    "\n",
    "print(\"Before Scaling\")\n",
    "print(\"Maxes: {}\".format(max(MAX_b)))\n",
    "print(\"Mins: {}\".format(min(MIN_b)))\n",
    "\n",
    "\n",
    "MAX_a = []\n",
    "MIN_a = []\n",
    "\n",
    "for i in range(len(Xk_std)):\n",
    "    MAX_a.append(max(Xk_std[i]))\n",
    "    MIN_a.append(min(Xk_std[i]))\n",
    "\n",
    "print('\\n')\n",
    "print(\"After Scaling\")\n",
    "print(\"Maxes: {}\".format(max(MAX_a)))\n",
    "print(\"Mins: {}\".format(min(MIN_a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive Feature Elimination\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols = linear_model.LinearRegression()\n",
    "rfecv = RFECV(estimator=ols,step=1,scoring=\"neg_mean_squared_error\")\n",
    "\n",
    "rfecv.fit(Xf, yf)\n",
    "rfecv.transform(Xf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfecv.n_features_, rfecv.support_, rfecv.ranking_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Principal Component Analysis\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.99, whiten=True)\n",
    "Xf_pca = pca.fit_transform(Xf)\n",
    "\n",
    "print(\"Original number of features:\", Xf.shape[1])\n",
    "print(\"Reduced number of features:\", Xf_pca.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA Kernel \n",
    "from sklearn.decomposition import PCA, KernelPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpca = KernelPCA(kernel=\"rbf\",gamma=15,n_components=1)\n",
    "Xf_kpca = kpca.fit_transform(Xf)\n",
    "\n",
    "print(\"Original number of features:\",Xf.shape[1])\n",
    "print(\"Reduced number of features:\",Xf_kpca.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# Linear Discriminant analysis\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LinearDiscriminantAnalysis(n_components=5)\n",
    "Xf_lda = lda.fit(Xf,yf).transform(Xf)\n",
    "\n",
    "print(\"Original number of features:\", Xf.shape[1])\n",
    "print(\"Reduced number of features:\", Xf_lda.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-negative matrix factorization\n",
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf = NMF(n_components=10,random_state=1)\n",
    "Xf_nmf = nmf.fit_transform(Xf)\n",
    "\n",
    "print(\"Original number of features:\", Xf.shape[1])\n",
    "print(\"Reduced number of features:\", Xf_nmf.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}